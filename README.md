# dl-driver

**A unified, high-performance AI/ML data loading framework with enterprise-grade capabilities**

[![Rust](https://img.shields.io/badge/rust-1.89.0+-blue.svg)](https://www.rust-lang.org)
[![Version](https://img.shields.io/badge/version-0.6.2-green.svg)](./docs/Changelog.md)
[![Build](https://img.shields.io/badge/build-passing-success.svg)](#compilation-status)
[![Formats](https://img.shields.io/badge/formats-3%20validated-brightgreen.svg)](#format-compatibility)
[![Validation](https://img.shields.io/badge/tests-21%2F21%20passing-success.svg)](#testing--validation)
[![Storage](https://img.shields.io/badge/storage-4%20backends-orange.svg)](#storage-backends)
[![Architecture](https://img.shields.io/badge/architecture-unified-blue.svg)](#architecture-overview)
[![REUSE status](https://api.reuse.software/badge/github.com/russfellows/dl-driver)](https://api.reuse.software/info/github.com/russfellows/dl-driver)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![ScanCode Compatible](https://img.shields.io/badge/ScanCode-Compatible-green.svg)](https://scancode.io/)

## ğŸš€ Overview

**dl-driver** is a production-ready AI/ML data loading framework that provides **100% format compatibility** with standard Python libraries. Built in Rust for performance and reliability, it serves as a drop-in replacement for [DLIO benchmarks](https://github.com/argonne-lcf/dlio_benchmark) while delivering enterprise-grade capabilities through the powerful [s3dlio](https://github.com/russfellows/s3dlio) library.

**Key Achievement**: Complete validation with numpy, h5py, and TensorFlow ensures seamless integration with existing ML pipelines.

## ğŸ¯ Current Status (v0.6.2)

**ğŸš€ TRUE DLIO PARALLEL I/O**: Revolutionary threading model with I/O/compute overlap
**âœ… STORAGE PERFORMANCE**: Realistic 4+ GiB/s throughput matching storage systems
**âš¡ ENTERPRISE SCALE**: Validated with 62.5GB datasets and 384 concurrent workers

### Latest v0.6.2 Release - TRUE DLIO Parallel I/O Revolution ğŸš€
- **âš¡ Parallel I/O Implementation**: TRUE DLIO-compatible parallel I/O + compute overlap using tokio channels
- **ğŸ“Š Corrected Throughput**: Fixed storage calculations from impossible 35TB/s to realistic 4.12 GiB/s
- **ğŸ¯ Realistic AU**: Accelerator Utilization now 42-50% (realistic) instead of 99% (impossible)
- **ğŸ”§ Background Workers**: 16-thread aggressive I/O with continuous batch prefetching (0.01ms retrieval!)
- **ğŸ“ˆ Large-Scale Support**: Re-enabled generate command, validated with 2000Ã—32MB files (62.5GB)

### v0.6.1 - Enterprise License Compliance
- **ğŸ“œ Complete REUSE 3.3 Compliance**: Professional SPDX headers across all 64+ source files
- **ğŸ” ScanCode Integration**: Automated license scanning with Docker-based validation
- **ğŸ·ï¸ GitHub Integration**: Compliance badges, documentation, and CI/CD workflows
- **ğŸ¯ Zero License Violations**: Clean enterprise-grade licensing implementation

### Major v0.6.0 Improvements
- **ğŸ—ï¸ Unified Command Interface**: Single `dl-driver run` command replaces fragmented dlio/mlperf/legacy commands
- **ğŸ¯ Simplified Architecture**: Removed artificial separation, all execution uses identical s3dlio core
- **ğŸ“Š Optional MLPerf Mode**: Enhanced reporting via `--mlperf` flag while maintaining standard DLIO execution
- **ğŸ§ª Comprehensive Test Matrix**: 21/21 tests passing across File, S3, and DirectIO backends
- **ğŸ”Œ Stable Plugin System**: CheckpointPlugin working identically across all modes and backends
- **âš¡ Performance Consistency**: Identical execution performance regardless of reporting mode

### Compilation Status
- **Full Workspace Compilation**: `cargo check --workspace` âœ… SUCCESS  
- **Release Builds**: `cargo build --release` âœ… SUCCESS
- **All Tests Passing**: 6/6 core library tests âœ… SUCCESS
- **CLI Fully Functional**: All commands operational âœ… SUCCESS

### Major Milestones Achieved
- **âœ… M5 - Checkpoint Plugin System**: Multi-backend checkpointing with optional zstd compression
- **âœ… M6 - MLPerf Production Readiness**: Provenance tracking, per-stage metrics, deterministic validation
- **ğŸ”§ Unified Configuration**: Single `DlioConfig` type with comprehensive validation
- **ğŸ”Œ Enterprise Plugin Architecture**: Async-capable plugin system with lifecycle management
- **ğŸ“¦ s3dlio v0.8.1 Integration**: Complete multi-backend support across all storage types

## ğŸš€ TRUE DLIO Parallel I/O Usage (v0.6.2)

### Complete Data Generation + Training Workflow

```bash
# Build dl-driver
cargo build --release

# Step 1: Generate large-scale dataset (separate phase, not measured)
./target/release/dl-driver generate --config tests/dlio_configs/large_scale_test.yaml

# Step 2: Run TRUE parallel I/O training (measured for AU calculation)
./target/release/dl-driver run --config tests/dlio_configs/large_scale_test.yaml
```

### Example Large-Scale DLIO Configuration
```yaml
# Large-scale parallel I/O configuration
model:
  framework: "tensorflow"

dataset:
  data_folder: "file:///mnt/vast1/my_large_dataset"  # Use high-capacity storage!
  format: "npz"
  num_files_train: 2000        # 2000 files for realistic scale
  record_length_bytes: 1048576 # 1MB per record
  num_samples_per_file: 32     # = 32MB per file, 64GB total

reader:
  data_loader: "tensorflow"
  read_threads: 16            # Aggressive parallel I/O workers
  prefetch: 4                 # Prefetch queue size
  batch_size: 16              # 16 files per batch = 512MB batches
  shuffle: false

train:
  epochs: 5
  computation_time: 0.05      # 50ms GPU simulation per batch

workload:
  workflow:
    generate_data: true       # For generate command
    train: true              # For run command
```

### Performance Expectations with TRUE Parallel I/O

**Expected Results:**
- **Storage Throughput**: 4-5 GiB/s (matches real storage systems)
- **I/O Time**: 0.01-0.02ms per batch (near-instant from prefetch queue)  
- **Compute Time**: 50ms per batch (GPU simulation)
- **AU (Accelerator Utilization)**: 42-50% (realistic parallel processing)
- **Background Workers**: 16 threads continuously loading batches

**Success Indicators:**
```
ğŸ“Š TIMING | Avg I/O: 0.01ms, Avg Compute: 51.2ms, AU: 45.2%
ğŸ‰ PARALLEL SUCCESS: I/O 0.0ms (near-instant!), AU 45.2% (realistic parallel)
Read throughput: 4222.85 MB/s (4.12 GiB/s) [STORAGE WALL-CLOCK]
```

### Legacy Commands (Still Supported)
```bash
# Validate DLIO configurations
./target/release/dl-driver validate --config tests/dlio_configs/minimal_config.yaml

# Run DLIO workloads (standard execution)
./target/release/dl-driver run --config <config> --pretty

# Run with MLPerf compliance reporting (same execution, enhanced metrics)
./target/release/dl-driver run --mlperf --config <config> --format json --max-epochs 5 --max-steps 2000

# Generate MLPerf reports with comprehensive analysis
./target/release/dl-driver run --mlperf --config <config> --format csv --output mlperf_report.csv
```

### âœ¨ Key Features

- **ğŸš€ TRUE DLIO Parallel I/O**: Revolutionary I/O+compute overlap with 16-thread background workers
- **ğŸ“Š Realistic Performance Metrics**: 4+ GiB/s storage throughput matching real storage systems  
- **âš¡ Enterprise-Scale Validation**: 62.5GB datasets, 384 concurrent workers, 0.01ms I/O times
- **ğŸ¯ Correct AU Calculation**: 42-50% Accelerator Utilization (realistic vs impossible 99%)
- **ğŸ¯ Complete AI/ML Format Compatibility**: Full validation with numpy, h5py, TensorFlow libraries
- **ğŸ”— DLIO Configuration Compatibility**: Drop-in replacement for existing DLIO YAML configs
- **ğŸ“‹ 3 Production Formats**: NPZ, HDF5, TFRecord with 100% standard library compatibility
- **ğŸª 4 Universal Storage Backends**: File, S3/MinIO, Azure Blob, DirectIO with unified interface  
- **ğŸ“„ Enterprise License Compliance**: REUSE 3.3 compliant, ScanCode validated, automated CI/CD scanning
- **ğŸ›¡ï¸ M5 Checkpoint Plugin System**: Multi-backend checkpointing with optional zstd compression
- **ğŸ“Š M6 MLPerf Production Readiness**: Provenance tracking, per-stage metrics, deterministic validation
- **ğŸ”§ Framework Integration**: PyTorch, TensorFlow, and JAX adapters with M4 Framework Profiles
- **â˜ï¸ Production Cloud Ready**: Real S3 and Azure credential support
- **ğŸ§ª Comprehensively Validated**: 60+ comprehensive tests with golden reference validation and MLCommons DLIO compatibility

## ï¿½ M5 & M6 Enterprise Features (NEW in v0.5.2) 

### M5: Checkpoint Plugin System
- **Multi-Backend Persistence**: Checkpoints work seamlessly across file://, directio://, s3://, and az:// storage
- **Configurable Compression**: Optional zstd compression reduces checkpoint artifact sizes
- **Async Plugin Architecture**: Complete lifecycle management with proper error handling
- **Automatic Integration**: Enable via `checkpoint.enabled: true` in DLIO config

### M6: MLPerf Production Readiness
- **Comprehensive Provenance**: Every report includes dl-driver and s3dlio version tracking
- **Per-Stage Timing Metrics**: Detailed I/O, decode, and host-to-device latency analysis  
- **Percentile Analysis**: P50, P95, P99 latencies for all performance stages
- **Deterministic Validation**: Access-order capture ensures reproducible benchmarks
- **Configurable Bounds**: `--max-epochs` and `--max-steps` CLI flags remove hardcoded limits

## ğŸ”§ M4 Framework Profiles

### Framework Integration Architecture
dl-driver now provides **comprehensive framework integration** with enterprise-grade ML/AI framework support:

**Supported Frameworks**:
- **PyTorch**: Full DataLoader adapter with configuration management
- **TensorFlow**: tf.data.Dataset configuration and pipeline support
- **JAX**: Framework configuration for JAX-based data pipelines

### Framework Configuration Examples

#### PyTorch Integration
```yaml
# DLIO config with embedded PyTorch framework profile
framework: pytorch
pytorch_config:
  batch_size: 64
  num_workers: 8
  shuffle: true
  pin_memory: true
  seed: 42

# Alternative: Framework Profiles structure
framework_profiles:
  pytorch:
    batch_size: 32
    num_workers: 4
    persistent_workers: true
```

#### TensorFlow Integration  
```yaml
framework: tensorflow
tensorflow_config:
  batch_size: 128
  shuffle_buffer_size: 10000
  prefetch_buffer_size: -1  # AUTOTUNE
  num_parallel_calls: -1    # AUTOTUNE
  deterministic: true
```

### Framework Features
- **Configuration Validation**: Comprehensive validation for all framework parameters
- **s3dlio Integration**: All frameworks leverage unified storage backends
- **Epoch Management**: Built-in epoch tracking (`current_epoch()`, `next_epoch()`, `reset_epoch()`)
- **Seed Management**: Reproducible training with seed state management
- **Format Detection**: Automatic format detection for framework compatibility

## ğŸ“Š Performance Benchmarks

### v0.3.0 - Advanced Data Loading Performance
**AsyncPoolDataLoader with Dynamic Batching** âš¡

| Backend | Files/Second | Processing Time | Configuration |
|---------|--------------|-----------------|---------------|
| **File** | **62,494** | 1.20ms | 24 pool, 6 workers |
| **S3/MinIO** | **44,831** | 1.67ms | 32 pool, 8 workers |
| **Azure** | **37,926** | 1.98ms | 28 pool, 7 workers |
| **DirectIO** | **23,061** | 3.25ms | 16 pool, 4 workers |

*Test conditions: 75 files per backend, 5 content types, microsecond batch latency*

## ğŸ¯ Format Compatibility

### v0.4.0 - AI/ML Format Validation âœ…
**100% Compatibility with Standard Python Libraries**

| Format | Library | Validation | Features |
|--------|---------|------------|----------|
| **NPZ** | `numpy.load()` | âœ… 12/12 tests | ZIP archives with `.npy` files |
| **HDF5** | `h5py.File()` | âœ… 12/12 tests | Hierarchical datasets, metadata |
| **TFRecord** | `tf.data.TFRecordDataset` | âœ… 12/12 tests | CRC-32C, protobuf varints |

**Total**: 36/36 format validations passed with standard Python AI/ML libraries!

### Format Implementation Details
- **NPZ**: s3dlio + zip library for proper numpy array archives
- **HDF5**: s3dlio + hdf5-metno for cross-platform compatibility  
- **TFRecord**: CRC-32C checksums + proper `tf.train.Example` encoding
- **Validation**: Comprehensive Python test suite in `tools/validation/`

## ğŸ† Key Achievements

### ğŸ¯ Production-Ready AI/ML Pipeline
dl-driver v0.4.0 represents a **major milestone** - complete transformation from a performance framework to a production-ready AI/ML data pipeline:

- **100% Format Compatibility**: All generated files work seamlessly with standard Python libraries
- **Enterprise Validation**: 36 comprehensive format tests ensure ongoing quality assurance
- **DLIO Drop-in Replacement**: Full MLCommons configuration compatibility with enhanced features
- **Multi-Backend Excellence**: Unified performance across File, S3, Azure, and DirectIO storage

### ğŸ“Š Validation Confidence
```
âœ… Framework Tests: 7/7 tests passing (PyTorch integration, validation, serialization)
âœ… Core Tests:     15/15 tests passing (DLIO parsing, workload management) 
âœ… Format Tests:    5/5 tests passing (NPZ, HDF5, TFRecord)
âœ… CLI Tests:      29/29 tests passing (configuration, backend integration)
âœ… Total Coverage: 56/56 comprehensive tests validating all functionality
```

## ğŸ—ï¸ Architecture

dl-driver follows a clean workspace architecture with 6 focused crates:

```
real_dlio/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ cli/          # Command-line interface
â”‚   â”œâ”€â”€ core/         # Workload orchestration and config parsing  
â”‚   â”œâ”€â”€ frameworks/   # Framework integrations (PyTorch, TensorFlow, JAX)
â”‚   â”œâ”€â”€ storage/      # Storage backend abstractions
â”‚   â”œâ”€â”€ formats/      # Data format handlers (HDF5, NPZ, etc.)
â”‚   â””â”€â”€ py_api/       # Python bindings (PyO3)
â”œâ”€â”€ tests/            # Integration and regression tests
â””â”€â”€ docs/             # Documentation and changelog
```

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/russfellows/dl-driver.git
cd dl-driver
cargo build --release
```

### Basic Usage

```bash
# Generate test datasets with different formats
./target/release/dl-driver generate --config tests/dlio_configs/minimal_config.yaml

# Run DLIO-compatible workloads (unified execution engine)
./target/release/dl-driver run --config tests/dlio_configs/unet3d_config.yaml

# Validate configuration without running
./target/release/dl-driver validate --config tests/dlio_configs/bert_config.yaml

# Run format validation (requires Python environment)
python tools/validation/validate_formats.py
```

### Command Overview
```bash
dl-driver --help                    # Show all available commands
dl-driver generate --help           # Generate synthetic datasets  
dl-driver run --help               # Run DLIO workloads (with optional MLPerf mode)
dl-driver validate --help          # Validate configurations
```

## ğŸª Storage Backends

This project provides unified access to multiple storage systems through URI schemes:

### File System (`file://`)
```yaml
dataset:
  data_folder: file:///tmp/my-workload/
```
- **Use Case**: Local filesystem testing, development
- **Performance**: 46+ MB/s throughput
- **Features**: Standard POSIX file operations

### S3 Compatible (`s3://`)  
```yaml
dataset:
  data_folder: s3://my-bucket/my-workload/
```
- **Use Case**: AWS S3, MinIO, S3-compatible object stores
- **Performance**: 20+ MB/s throughput  
- **Authentication**: AWS credentials, .env file support

### Azure Blob Storage (`az://`)
```yaml  
dataset:
  data_folder: az://storage-account/container/path/
```
- **Use Case**: Azure cloud storage
- **Authentication**: Azure CLI, service principal
- **Features**: Integrated with Azure SDK

### DirectIO (`direct://`)
```yaml
dataset:  
  data_folder: direct:///tmp/high-perf-workload/
```
- **Use Case**: High-performance applications, HPC workloads
- **Performance**: 85+ MB/s throughput
- **Features**: O_DIRECT, zero-copy I/O, automatic fallback

## ğŸ“ Configuration

dl-driver provides **complete MLCommons DLIO compatibility** with enhanced format support:

```yaml
# Example DLIO-compatible configuration
model:
  name: unet3d_workload

framework: pytorch

workflow:
  generate_data: true
  train: true  
  checkpoint: false

dataset:
  data_folder: file:///mnt/vast1/workload-data/  # Use large data directories
  format: tfrecord                              # NPZ, HDF5, or TFRecord
  num_files_train: 100
  record_length_bytes: 1048576                  # 1MB files

reader:
  data_loader: pytorch
  batch_size: 32
  read_threads: 4
  prefetch: 8
  shuffle: true

train:
  epochs: 10
  computation_time: 0.1
```

### Supported Formats
- **NPZ**: NumPy array archives - `format: npz`
- **HDF5**: Hierarchical data format - `format: hdf5`  
- **TFRecord**: TensorFlow records - `format: tfrecord`

### Configuration Examples
- `tests/dlio_configs/minimal_config.yaml` - Basic setup
- `tests/dlio_configs/unet3d_config.yaml` - UNet3D benchmark
- `tests/dlio_configs/bert_config.yaml` - BERT training config
- `tests/dlio_configs/resnet_config.yaml` - ResNet configuration

## ğŸ§ª Testing & Validation

### Comprehensive Test Suite

```bash
# Run all integration tests (45 tests)
cargo test

# Run format validation with Python libraries (36 tests)
python tools/validation/validate_formats.py

# Test specific backend (requires credentials)
AZURE_BLOB_ACCOUNT=myaccount cargo test test_azure_backend
S3_ENDPOINT=http://localhost:9000 cargo test test_s3_backend
```

### Validation Results
- âœ… **45/45 Rust integration tests** passing
- âœ… **36/36 format validation tests** with Python libraries
- âœ… **100% compatibility** with numpy, h5py, tensorflow
- âœ… **MLCommons DLIO configs** fully validated

### Test Categories
- **Backend Integration**: File, S3, Azure, DirectIO validation
- **Format Compatibility**: NPZ, HDF5, TFRecord with standard libraries
- **DLIO Compliance**: Configuration parsing and workload execution
- **Performance**: s3dlio AsyncPoolDataLoader benchmarks

## ğŸ› ï¸ Development

### Prerequisites
- Rust 1.89.0 or later
- s3dlio library (automatically handled by Cargo)

### Building from Source
```bash
git clone https://github.com/russfellows/dl-driver.git
cd dl-driver
cargo build --release
```

### Contributing
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality  
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“ˆ Roadmap

### âœ… Completed (v0.4.0) - AI/ML Format Compatibility
- âœ… **Complete format compatibility**: NPZ, HDF5, TFRecord with 100% Python library validation
- âœ… **Enhanced s3dlio integration**: Unified data generation and advanced AsyncPoolDataLoader
- âœ… **Comprehensive validation framework**: 36 format tests + 45 integration tests
- âœ… **MLCommons DLIO compliance**: Full configuration support and parsing validation
- âœ… **Professional project structure**: Proper Rust conventions, documentation, testing

### âœ… Previous Releases
**v0.3.0**: Enterprise-grade performance (62K+ files/second), dynamic batching, auto-tuning
**v0.2.0**: Multi-backend storage (File, S3, Azure, DirectIO), DLIO configuration compatibility

### ï¿½ Future Enhancements (v0.5.3+)
- **Additional formats**: Parquet, Arrow for modern data science workflows
- **Enhanced Python bindings**: Complete PyO3 API for Python integration
- **Golden reference validation**: Automated tolerance-based benchmark validation
- **Distributed coordination**: Multi-node workload orchestration  
- **Advanced profiling**: Extended I/O and compute metrics beyond current per-stage timing
- **Cloud-native features**: Kubernetes integration, auto-scaling

## ğŸ“š Documentation

- [Changelog](./docs/Changelog.md) - Detailed version history
- [Configuration Guide](./tests/configs/) - Example configurations
- [API Documentation](https://docs.rs/real_dlio) - Rust API docs

## ğŸ¤ Acknowledgments

- [DLIO Benchmark](https://github.com/argonne-lcf/dlio_benchmark) - Original inspiration and configuration format
- [s3dlio](https://github.com/russfellows/s3dlio) - Powerful multi-backend storage library
- Rust ecosystem - tokio, serde, anyhow, and many other excellent crates

## ğŸ“„ License & Compliance

This project maintains **enterprise-grade license compliance** with comprehensive scanning and validation.

### License Information
- **License**: [GPL-3.0-or-later](LICENSES/GPL-3.0-or-later.txt) 
- **REUSE Compliant**: Full compliance with [REUSE Specification 3.3](https://reuse.software/spec/)
- **SPDX Standards**: All source files include proper SPDX license identifiers
- **ScanCode Compatible**: Validated with ScanCode Toolkit for enterprise scanning

### Compliance Summary
- âœ… **201 files scanned** by ScanCode Toolkit
- âœ… **72 files** with SPDX GPL-3.0 identifiers  
- âœ… **80 files** with proper copyright attribution
- âœ… **Automated CI/CD** license validation via GitHub Actions

ğŸ“‹ **[View Detailed Compliance Report](docs/LICENSE-COMPLIANCE.md)**

### Local Validation
```bash
# REUSE compliance check
reuse lint

# ScanCode analysis (via Docker)
docker run --rm -v $(pwd):/workdir sixarm/scancode \
  --copyright --license --package --info --license-text \
  --strip-root --format html-app /workdir /workdir/compliance-report.html
```

---

